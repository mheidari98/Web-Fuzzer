{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hired-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from requests_html import HTMLSession\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import colorama\n",
    "import random\n",
    "from stem.control import Controller\n",
    "from stem import Signal\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "trained-valley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sys.executable)\n",
    "# print(sys.version)\n",
    "# print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "checked-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the colorama module\n",
    "colorama.init()\n",
    "GREEN = colorama.Fore.GREEN\n",
    "GRAY = colorama.Fore.LIGHTBLACK_EX\n",
    "RESET = colorama.Fore.RESET\n",
    "YELLOW = colorama.Fore.YELLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "spoken-mortality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid(url):\n",
    "    \"\"\"\n",
    "    Checks whether `url` is a valid URL.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
    "\n",
    "\n",
    "def get_all_website_links(url, DynamicSite=0, verbose=False):\n",
    "    \"\"\"\n",
    "    Returns all URLs that is found on `url` in which it belongs to the same website\n",
    "    \"\"\"\n",
    "    \n",
    "    urls = set()     # all URLs of `url`\n",
    "    \n",
    "    domain_name = urlparse(url).netloc     # domain name of the URL without the protocol\n",
    "\n",
    "    if DynamicSite:\n",
    "        session = HTMLSession()   # initialize an HTTP session\n",
    "        \n",
    "        response = session.get(url)  # make HTTP request & retrieve response\n",
    "        \n",
    "        # execute Javascript\n",
    "        try:\n",
    "            response.html.render()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        html_doc = response.html.html\n",
    "        \n",
    "    else :\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        html_doc = r.content  # r.content    r.text\n",
    "    \n",
    "\n",
    "    \n",
    "    soup = BeautifulSoup( html_doc , \"html5lib\")  # 'html5lib' , 'html.parser'  'lxml'\n",
    "    \n",
    "    \n",
    "    #for link in soup.find_all(attrs={'href': re.compile(\"http\")}):\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        \n",
    "        href = link.get('href')     #  link.attrs.get(\"href\")   \n",
    "        \n",
    "        if href == \"\" or href is None:   \n",
    "            continue   # href empty tag\n",
    "            \n",
    "        href = urljoin(url, href)      # join the URL if it's relative (not absolute link)\n",
    "        \n",
    "        parsed_href = urlparse(href)\n",
    "        \n",
    "        \n",
    "        href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path  # remove URL GET parameters, URL fragments, etc.\n",
    "        \n",
    "        if not is_valid(href):     # not a valid URL\n",
    "            continue      \n",
    "            \n",
    "        if href in internal_urls:  # already in the set\n",
    "            continue       \n",
    "            \n",
    "        if domain_name not in href:  # external link\n",
    "            if href not in external_urls:\n",
    "                if verbose :\n",
    "                    print(f\"{GRAY}[!] External link: {href}{RESET}\")\n",
    "                external_urls.add(href)\n",
    "            continue\n",
    "            \n",
    "        if verbose :\n",
    "            print(f\"{GREEN}[*] Internal link: {href}{RESET}\")\n",
    "        internal_urls.add(href)\n",
    "        \n",
    "        urls.add(href)\n",
    "        \n",
    "    return urls\n",
    "\n",
    "\n",
    "def crawl(url, max_urls=30, DynamicSite=0, verbose=False):\n",
    "    \"\"\"\n",
    "    Crawls a web page and extracts all links.\n",
    "    You'll find all links in `external_urls` and `internal_urls` global set variables.\n",
    "    params:\n",
    "        max_urls (int): number of max urls to crawl, default is 30.\n",
    "    \"\"\"\n",
    "    global total_urls_visited\n",
    "    total_urls_visited += 1\n",
    "    print(f\"{YELLOW}[*] Crawling: {url}{RESET}\")\n",
    "    links = get_all_website_links(url, DynamicSite, verbose)\n",
    "    for link in links:\n",
    "        if total_urls_visited > max_urls:\n",
    "            break\n",
    "        crawl(link, max_urls, DynamicSite, verbose )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "expanded-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://www.geeksforgeeks.org/\"\n",
    "# url = \"https://www.varzesh3.com/\"\n",
    "url = \"https://iut.ac.ir/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dress-possibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Crawling: https://iut.ac.ir/\n",
      "[*] Crawling: http://news.iut.ac.ir/node/30645 \n",
      "[*] Crawling: http://news.iut.ac.ir/contact-us\n",
      "[*] Crawling: http://news.iut.ac.ir/send-news\n",
      "[*] Crawling: http://news.iut.ac.ir/node/add\n",
      "[*] Crawling: http://news.iut.ac.ir/archive-announcements\n",
      "[*] Crawling: http://news.iut.ac.ir/content/%D8%AB%D8%A8%D8%AA%E2%80%8C%D9%86%D8%A7%D9%85-%D8%A2%D8%B2%D9%85%D9%88%D9%86-%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85%DB%8C-%D8%B3%D8%A7%D9%84-%DB%B1%DB%B4%DB%B0%DB%B0-%D8%AF%D8%A7%D9%86%D8%B4%DA%AF%D8%A7%D9%87%E2%80%8C%D9%87%D8%A7-%D9%88-%D9%85%D8%A4%D8%B3%D8%B3%D8%A7%D8%AA-%D9%88%D8%B2%D8%A7%D8%B1%D8%AA-%D8%B9%D9%84%D9%88%D9%85%D8%8C-%D8%AA%D8%AD%D9%82%DB%8C%D9%82%D8%A7%D8%AA-%D9%88-%D9%81%D9%86%D8%A2%D9%88%D8%B1%DB%8C\n",
      "[*] Crawling: http://news.iut.ac.ir/content/%D8%A7%D8%B7%D9%84%D8%A7%D8%B9%DB%8C%D9%87-%D9%86%DA%AF%D8%B1%D8%B4-%D8%B3%D9%86%D8%AC%DB%8C-%D8%AF%D8%B1%D9%88%D8%B3-%D8%AF%D8%B1-%D9%86%DB%8C%D9%85%D8%B3%D8%A7%D9%84-%DB%B2-%DB%B9%DB%B9\n",
      "[*] Crawling: http://news.iut.ac.ir/content/%D8%A7%D8%B7%D9%84%D8%A7%D8%B9%D9%8A%D9%87-%D9%85%D8%B9%D8%A7%D9%88%D9%86%D8%AA-%D8%A2%D9%85%D9%88%D8%B2%D8%B4%D9%8A-%D8%AF%D8%A7%D9%86%D8%B4%DA%AF%D8%A7%D9%87-%D8%AF%D8%B1%D8%AE%D8%B5%D9%88%D8%B5-%D8%A7%D9%85%D8%AA%D8%AD%D8%A7%D9%86%D8%A7%D8%AA-%D9%BE%D8%A7%D9%8A%D8%A7%D9%86-%D9%86%D9%8A%D9%85%D8%B3%D8%A7%D9%84-%D8%AC%D8%A7%D8%B1%D9%8A\n",
      "[*] Crawling: http://news.iut.ac.ir/content/%D8%AB%D8%A8%D8%AA-%D9%86%D8%A7%D9%85-%D8%AF%D8%B1-%D8%B7%D8%B1%D8%AD-%D8%A7%D8%B2%D8%AF%D9%88%D8%A7%D8%AC-%D8%AF%D8%A7%D9%86%D8%B4%D8%AC%D9%88%D9%8A%D9%8A-%D9%87%D9%85%D8%B3%D9%81%D8%B1-%D8%AA%D8%A7-%D8%A8%D9%87%D8%B4%D8%AA-%D8%A2%D8%BA%D8%A7%D8%B2-%D8%B4%D8%AF\n",
      "[*] Crawling: http://news.iut.ac.ir/content/%D8%AA%D9%82%D9%88%DB%8C%D9%85-%D8%A2%D9%85%D9%88%D8%B2%D8%B4%DB%8C-%D8%AF%D8%A7%D9%86%D8%B4%DA%AF%D8%A7%D9%87-%D8%B5%D9%86%D8%B9%D8%AA%D9%8A-%D8%A7%D8%B5%D9%81%D9%87%D8%A7%D9%86-%D9%86%D9%8A%D9%85%D8%B3%D8%A7%D9%84-%D8%A7%D9%88%D9%84-%D8%B3%D8%A7%D9%84-%D8%AA%D8%AD%D8%B5%D9%8A%D9%84%D9%8A1401-1400\n"
     ]
    }
   ],
   "source": [
    "# initialize the set of links (unique links)\n",
    "internal_urls = set()\n",
    "external_urls = set()\n",
    "\n",
    "total_urls_visited = 0\n",
    "\n",
    "crawl(url, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-motor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-venezuela",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description=\"Link Extractor Tool with Python\")\n",
    "    parser.add_argument(\"url\", help=\"The URL to extract links from.\")\n",
    "    parser.add_argument(\"-m\", \"--max-urls\", help=\"Number of max URLs to crawl, default is 30.\", default=30, type=int)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    url = args.url\n",
    "    max_urls = args.max_urls\n",
    "\n",
    "    crawl(url, max_urls=max_urls)\n",
    "\n",
    "    print(\"[+] Total Internal links:\", len(internal_urls))\n",
    "    print(\"[+] Total External links:\", len(external_urls))\n",
    "    print(\"[+] Total URLs:\", len(external_urls) + len(internal_urls))\n",
    "    print(\"[+] Total crawled URLs:\", max_urls)\n",
    "\n",
    "    domain_name = urlparse(url).netloc\n",
    "\n",
    "    # save the internal links to a file\n",
    "    with open(f\"{domain_name}_internal_links.txt\", \"w\") as f:\n",
    "        for internal_link in internal_urls:\n",
    "            print(internal_link.strip(), file=f)\n",
    "\n",
    "    # save the external links to a file\n",
    "    with open(f\"{domain_name}_external_links.txt\", \"w\") as f:\n",
    "        for external_link in external_urls:\n",
    "            print(external_link.strip(), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-china",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-experience",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-institute",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-approach",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sublist3r "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# engines='baidu,yahoo,google,bing,ask,netcraft,dnsdumpster,virustotal,threatcrowd,ssl,passivedns'\n",
    "\n",
    "subdomains = sublist3r.main(domain='varzesh3.com', \n",
    "                            threads=40, \n",
    "                            savefile='yahoo_subdomains.txt', \n",
    "                            ports= None, \n",
    "                            silent=False, \n",
    "                            verbose= False, \n",
    "                            enable_bruteforce= False, \n",
    "                            engines=None\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-better",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-belief",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "traditional-gross",
   "metadata": {},
   "source": [
    "## Using Tor as a Proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-attribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tor_session():\n",
    "    session = requests.session()\n",
    "    # Tor uses the 9050 port as the default socks port\n",
    "    session.proxies = {'http':  'socks5://127.0.0.1:9050',\n",
    "                       'https': 'socks5://127.0.0.1:9050'}\n",
    "    return session\n",
    "\n",
    "def renew_connection():\n",
    "    with Controller.from_port(port=9051) as c:\n",
    "        c.authenticate()\n",
    "        # send NEWNYM signal to establish a new clean connection through the Tor network\n",
    "        c.signal(Signal.NEWNYM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-messenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "torport = 9050\n",
    "proxies = {\n",
    "    'http': \"socks5h://localhost:{}\".format(torport),\n",
    "    'https': \"socks5h://localhost:{}\".format(torport)\n",
    "}\n",
    "\n",
    "print(requests.get('http://icanhazip.com', proxies=proxies).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-static",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a request through the Tor connection\n",
    "# IP visible through Tor\n",
    "session = get_tor_session()\n",
    "print(session.get(\"http://httpbin.org/ip\").text)\n",
    "# Above should print an IP different than your public IP\n",
    "\n",
    "# Following prints your normal public IP\n",
    "print(requests.get(\"http://httpbin.org/ip\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-orange",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    s = get_tor_session()\n",
    "    ip = s.get(\"http://icanhazip.com\").text\n",
    "    print(\"IP:\", ip)\n",
    "    renew_connection()\n",
    "    s = get_tor_session()\n",
    "    ip = s.get(\"http://icanhazip.com\").text\n",
    "    print(\"IP:\", ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-classic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/clarketm/proxy-list\n",
    "def get_free_proxies():\n",
    "    proxyURL = \"https://raw.githubusercontent.com/clarketm/proxy-list/master/proxy-list-raw.txt\"\n",
    "\n",
    "    data = urllib.request.urlopen(proxyURL).read( 20*400 ).split()\n",
    "\n",
    "    proxies = [ line.decode(\"utf-8\") for line in data ]\n",
    "\n",
    "    return proxies\n",
    "\n",
    "\n",
    "def get_free_proxies2():\n",
    "    url = \"https://free-proxy-list.net/\"\n",
    "    # get the HTTP response and construct soup object\n",
    "    soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "    proxies = []\n",
    "    for row in soup.find(\"table\", attrs={\"id\": \"proxylisttable\"}).find_all(\"tr\")[1:]:\n",
    "        tds = row.find_all(\"td\")\n",
    "        try:\n",
    "            ip = tds[0].text.strip()\n",
    "            port = tds[1].text.strip()\n",
    "            host = f\"{ip}:{port}\"\n",
    "            proxies.append(host)\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return proxies\n",
    "\n",
    "def get_session(proxies):\n",
    "    # construct an HTTP session\n",
    "    session = requests.Session()\n",
    "    # choose one random proxy\n",
    "    proxy = random.choice(proxies)\n",
    "    session.proxies = {\"http\": 'http://'+proxy, \"https\": 'https://'+proxy}\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-measure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-market",
   "metadata": {},
   "outputs": [],
   "source": [
    "proxies = get_free_proxies()\n",
    "\n",
    "for i in range(100):\n",
    "    s = get_session(proxies)\n",
    "    try:\n",
    "        print(\"Request page with IP:\", s.get(\"http://icanhazip.com\", timeout=1.5).text.strip())\n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-insider",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-guide",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-territory",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-receiver",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-civilization",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-speaking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-cause",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-accordance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-favorite",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-medicaid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-cedar",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.geeksforgeeks.org\"\n",
    "# url = \"https://www.varzesh3.com/\"\n",
    "url = \"https://iut.ac.ir/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = urlparse(url)\n",
    "parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "urljoin(\"https://iut.ac.ir/ab/\", \"http://indust.iut.ac.ir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-divide",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-lobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( len( soup.find_all('a', href=True) ) )\n",
    "print( len( soup.find_all(attrs={'href': re.compile(\"http\")}) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-chancellor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnvSSD",
   "language": "python",
   "name": "envssd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
